%  LaTeX support: latex@mdpi.com 
%=================================================================
\documentclass[ai,article,submit,pdftex,moreauthors]{Definitions/mdpi} 

%=================================================================
% MDPI internal commands - do not modify
\firstpage{1} 
\makeatletter 
\setcounter{page}{\@firstpage} 
\makeatother
\pubvolume{1}
\issuenum{1}
\articlenumber{0}
\pubyear{2025}
\copyrightyear{2025}
\datereceived{ } 
\daterevised{ }
\dateaccepted{ } 
\datepublished{ } 

%=================================================================
% TikZ for architecture diagram
\usetikzlibrary{arrows.meta, positioning}

%=================================================================
% Full title of the paper
\Title{DCGAN-Based Dark Skin Image Augmentation for Improving Dermatology Classifier Fairness: An End-to-End Clinical Decision Support System}

% Author Orchid ID
\newcommand{\orcidauthorA}{0009-0006-0048-5767}

% Authors
\Author{Ogwumike Ugochukwu Belusochim Phillips $^{1,}$*\orcidA{}}

% MDPI internal command: Authors, for metadata in PDF
\AuthorNames{Ogwumike Ugochukwu Belusochim Phillips}

% Affiliations
\address{%
$^{1}$ \quad Illinois Institute of Technology, Chicago, IL, USA; bogwumike@hawk.illinoistech.edu}

% Corresponding author
\corres{Correspondence: bogwumike@hawk.illinoistech.edu}

% Abstract
\abstract{Despite achieving dermatologist-level accuracy, deep learning models for skin lesion classification exhibit persistent performance disparities across skin tones, with dark-skinned individuals (Fitzpatrick IV--VI) experiencing significantly lower diagnostic accuracy due to severe underrepresentation in training datasets like HAM10000 ($<$5\% dark-skin samples). We present a comprehensive approach to fair skin lesion classification that achieves 85\% accuracy through strategic class resampling and demonstrates improved demographic equity. Our EfficientNetB0 classifier, trained on HAM10000's 7-class dataset with resampling to 5,000 samples per class, achieves strong baseline performance (85\% accuracy, 0.85 macro-F1) while addressing severe class imbalance (58:1 ratio). Through systematic evaluation including fairness metrics, we show that our approach reduces performance disparities: GAN-augmented models improve fairness (Equalized Odds difference: 0.13 $\rightarrow$ 0.07) by enhancing dark-skin performance, though with a tradeoff in overall accuracy. We investigate DCGAN-based synthetic augmentation with 40 identified dark-skin training images, generating 3,500 synthetic samples (500 per disease class), and establish minimum data requirements for effective medical image synthesis. Our findings demonstrate that strategic resampling provides a robust foundation for fair dermatology AI, while synthetic augmentation requires careful validation. We deploy our complete pipeline as an integrated clinical decision support system with Gemini-powered patient education and geolocation-based care navigation, demonstrating practical translation from research to accessible healthcare tools.}

% Keywords
\keyword{medical imaging; generative adversarial networks; fairness in AI; dermatology; skin lesion classification; dataset bias; clinical decision support systems; data augmentation; EfficientNet; demographic equity} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

Deep learning models for dermatology diagnosis achieve dermatologist-level accuracy on standardized datasets~\cite{esteva2017, haenssle2018}, yet \textit{consistently underperform on dark-skinned individuals}~\cite{adamson2018, daneshjou2022}. This disparity stems from severe dataset imbalance: widely-used datasets like HAM10000~\cite{tschandl2018} contain $<$5\% dark-skin images (Fitzpatrick IV--VI), causing models to exhibit high accuracy on light-skinned groups but significantly lower sensitivity on darker skin tones~\cite{kinyanjui2020}. The clinical consequences are severe: skin cancer in dark-skinned populations is often diagnosed at later stages with worse outcomes, partly due to biased AI tools~\cite{agbai2014}.

Collecting diverse training data faces multiple barriers: limited imaging device access in underserved regions, lower historical enrollment in clinical studies, privacy constraints, and geographic concentration of research in predominantly light-skinned populations~\cite{wen2022}. We address this challenge through strategic class resampling and DCGAN-based synthetic augmentation, systematically evaluating whether GAN augmentation can mitigate bias when training data is extremely limited (40 images).

Our investigation achieves 85\% accuracy through strategic resampling and demonstrates improved demographic equity. We deploy a complete clinical decision support system integrating EfficientNetB0 classification, Gemini-powered patient education, and geolocation-based care navigation, bridging the gap between research and clinical utility.

\subsection{Contributions}

This work makes the following key contributions:
\begin{enumerate}[leftmargin=*, itemsep=2pt]
    \item \textbf{High-performance fair classifier}: 85\% accuracy (0.85 macro-F1) on HAM10000's 7-class classification through strategic resampling, demonstrating effective class imbalance mitigation.
    \item \textbf{Improved demographic equity}: GAN augmentation improves fairness (Equalized Odds: 0.13 $\rightarrow$ 0.07) by enhancing dark-skin performance, with tradeoffs requiring context-dependent deployment.
    \item \textbf{Evaluation framework}: Rigorous validation framework for synthetic medical data including accuracy, fairness metrics, and statistical testing, providing actionable guidance.
    \item \textbf{Data requirements}: Minimum thresholds ($>$100--200 samples) for effective medical GAN augmentation, established through systematic investigation with 40 training images.
    \item \textbf{Clinical deployment}: Production-ready system integrating classification, patient education, and care navigation, demonstrating practical translation to healthcare tools.
    \item \textbf{Reproducibility}: Fully documented open-source codebase enabling community validation and extension.
\end{enumerate}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Related Work}

\subsection{Deep Learning in Dermatology}

Since Esteva et al.'s landmark work demonstrating dermatologist-level skin cancer classification~\cite{esteva2017}, deep learning has advanced rapidly. EfficientNet~\cite{tan2019} emerged as a popular choice for its parameter efficiency through compound scaling. Haenssle et al.~\cite{haenssle2018} found CNNs superior to 58 dermatologists in sensitivity, yet these successes mask a critical issue: validation is primarily on light-skinned populations.

\subsection{Bias and Fairness in Medical AI}

Algorithmic bias in healthcare is a critical concern. Obermeyer et al.~\cite{obermeyer2019} found commercial healthcare algorithms exhibit significant racial bias, while Adamson and Smith~\cite{adamson2018} identified lack of dataset diversity as a fundamental cause of disparate dermatology AI performance. Daneshjou et al.~\cite{daneshjou2022} demonstrated consistent performance degradation on darker skin across dermatology AI systems. The Fitzpatrick17k dataset~\cite{groh2021} enables fairness evaluation but remains smaller than HAM10000.

Key fairness metrics~\cite{hardt2016} include: \textbf{Equalized Odds} (equal TPR and FPR across groups), \textbf{Demographic Parity} (equal positive prediction rates), and \textbf{Calibration} (equal probability that positive predictions are correct).

\subsection{Addressing Dataset Bias}

Strategies for mitigating bias include: (1) \textbf{Data collection}---direct but slow due to logistical constraints~\cite{codella2018}; (2) \textbf{Resampling}---SMOTE for tabular data~\cite{chawla2002}, oversampling with augmentation for images~\cite{wong2016}; (3) \textbf{Re-weighting}---adjusting loss weights to emphasize minority classes~\cite{cui2019}; (4) \textbf{Domain adaptation}---aligning feature distributions across domains~\cite{ganin2016}.

\subsection{Generative Adversarial Networks in Medical Imaging}

GANs~\cite{goodfellow2014} have enabled medical image synthesis for CT-to-MRI translation~\cite{wolterink2017}, stain normalization~\cite{shaban2019}, rare condition augmentation~\cite{frid2018}, and super-resolution~\cite{mahapatra2019}. DCGAN~\cite{radford2015} introduced convolutional layers for stable training, while CycleGAN~\cite{zhu2017} enables unpaired image translation---essential for skin-tone translation since paired lesion images across skin tones don't exist. Recent work explores StyleGAN~\cite{karras2019} and diffusion models~\cite{ho2020} for higher quality but requiring more data and computation.

Fairness-aware augmentation has targeted bias mitigation: Glocker et al.~\cite{glocker2023} use balanced sampling for chest X-ray fairness, while Puyol-Ant√≥n et al.~\cite{puyol2021} employ adversarial training for demographic-invariant cardiac MRI representations. Few-shot approaches (prototypical networks~\cite{snell2017}, MAML~\cite{finn2017}) offer alternatives with $<$50 samples per class. We select DCGAN for its efficiency and smaller data requirements, though our results show it still needs $>$40 samples for effective augmentation.

\subsection{Clinical Decision Support Systems}

Effective CDSS must provide interpretable outputs, workflow integration, uncertainty quantification, and patient-facing explanations~\cite{sutton2020}. Recent LLM advances enable sophisticated medical chatbots achieving physician-level performance on medical Q\&A~\cite{singhal2023}, though careful prompt engineering and safety guardrails remain essential.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Dataset}

\subsection{HAM10000 Overview}

We utilize the HAM10000 dataset~\cite{tschandl2018}, a publicly available collection of 10,015 dermoscopic images acquired using different modalities (dermoscopy and VivaScope). The dataset spans seven diagnostic classes with significant class imbalance (Table~\ref{tab:dataset}).

\begin{table}[H]
\caption{HAM10000 Class Distribution and Clinical Significance.\label{tab:dataset}}
\begin{tabularx}{\textwidth}{llCC}
\toprule
\textbf{Class} & \textbf{Full Name} & \textbf{Count} & \textbf{\%} \\
\midrule
nv & Melanocytic nevi & 6,705 & 66.95 \\
mel & Melanoma & 1,113 & 11.11 \\
bkl & Benign keratosis & 1,099 & 10.97 \\
bcc & Basal cell carcinoma & 514 & 5.13 \\
akiec & Actinic keratosis & 327 & 3.27 \\
vasc & Vascular lesions & 142 & 1.42 \\
df & Dermatofibroma & 115 & 1.15 \\
\midrule
& \textbf{Total} & \textbf{10,015} & \textbf{100.00} \\
\bottomrule
\end{tabularx}
\end{table}

The class imbalance ratio (majority to minority) is 58:1, presenting significant challenges for classifier training. Additionally, the dataset lacks explicit Fitzpatrick skin tone labels, though based on metadata analysis and prior literature~\cite{kinyanjui2020}, we estimate $>$95\% of images represent Fitzpatrick types I--III.

\subsection{Clinical Context}

Classes range from life-threatening (melanoma: 99\% to 27\% five-year survival from localized to metastatic~\cite{siegel2023}) to benign (melanocytic nevi, benign keratosis, dermatofibroma, vascular lesions). Basal cell carcinoma is most common but rarely metastasizes. Actinic keratosis is pre-cancerous (10\% progress to squamous cell carcinoma if untreated).

\subsection{Data Preprocessing}

Preprocessing pipeline: (1) resizing to $256 \times 256$ (GAN) and $224 \times 224$ (classifier); (2) hair removal using DullRazor algorithm~\cite{lee1997}; (3) histogram equalization for color normalization; (4) ImageNet normalization (mean $= [0.485, 0.456, 0.406]$, std $= [0.229, 0.224, 0.225]$). Stratified train/val/test split (70\%/15\%/15\%, random state=42) preserves class distributions. After resampling to 5,000 samples per class, training contains 24,500 images; validation and test each contain 5,250 images.

\begin{figure}[H]
\includegraphics[width=\textwidth]{class_distribution.png}
\caption{Class distribution in the HAM10000 dataset demonstrating severe class imbalance across seven dermatological conditions. Melanocytic nevi (nv, benign moles) dominate with 6,705 samples (67\% of dataset), while rare conditions like dermatofibroma (df, 115 samples, 1.15\%) and vascular lesions (vasc, 142 samples, 1.42\%) are severely underrepresented. The majority-to-minority class ratio of 58:1 presents significant challenges for classifier training and motivates our strategic resampling approach to 5,000 samples per class.\label{fig:dataset}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods}

\subsection{System Architecture Overview}

Our system comprises four integrated modules (Figure~\ref{fig:architecture}):
\begin{enumerate}[leftmargin=*, itemsep=2pt]
    \item \textbf{DCGAN Module}: Generates synthetic dark-skin images
    \item \textbf{Classification Module}: EfficientNetB0-based diagnosis
    \item \textbf{Chatbot Module}: Gemini-powered patient education
    \item \textbf{Locator Module}: Geolocation-based hospital finder
\end{enumerate}

\begin{figure}[H]
\includegraphics[width=\textwidth]{gan_architecture.png}
\caption{End-to-end system architecture showing the complete pipeline from data preprocessing to deployment. The training pipeline (top) combines DCGAN-generated synthetic dark-skin images with strategically resampled HAM10000 data to train the EfficientNetB0 classifier. The inference pipeline (bottom) integrates three components: (1) skin lesion classification with confidence scores, (2) Gemini-powered chatbot for patient education about diagnosed conditions, and (3) geolocation-based hospital finder for connecting users with nearby dermatology care.\label{fig:architecture}}
\end{figure}

\subsection{DCGAN for Synthetic Dark-Skin Image Generation}

We employ DCGAN~\cite{radford2015} for generating synthetic dark-skin images, using convolutional layers in both generator and discriminator for stable training:

\textbf{Generator}: Transforms 100-dimensional noise vector into 64$\times$64$\times$3 images:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Dense(8$\times$8$\times$256) $\rightarrow$ BatchNormalization $\rightarrow$ LeakyReLU
    \item Reshape to (8, 8, 256)
    \item Conv2DTranspose(128, 5$\times$5, stride=2) $\rightarrow$ BatchNorm $\rightarrow$ LeakyReLU $\rightarrow$ (16, 16, 128)
    \item Conv2DTranspose(64, 5$\times$5, stride=2) $\rightarrow$ BatchNorm $\rightarrow$ LeakyReLU $\rightarrow$ (32, 32, 64)
    \item Conv2DTranspose(3, 5$\times$5, stride=2, activation='tanh') $\rightarrow$ (64, 64, 3)
\end{itemize}

\textbf{Discriminator}: Classifies images as real or fake:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Input: (64, 64, 3)
    \item Conv2D(64, 5$\times$5, stride=2) $\rightarrow$ LeakyReLU $\rightarrow$ Dropout(0.3)
    \item Conv2D(128, 5$\times$5, stride=2) $\rightarrow$ LeakyReLU $\rightarrow$ Dropout(0.3)
    \item Flatten()
    \item Dense(1) - no activation (logits)
\end{itemize}

\subsubsection{Loss Functions}

Both networks use binary cross-entropy loss with label smoothing (0.9 for real, 0.1 for fake) to stabilize training:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Generator loss}: $\mathcal{L}_G = -\mathbb{E}[\log D(G(z))]$ where $z \sim \mathcal{N}(0,1)$ and $D(G(z))$ is the discriminator's probability that generated images are real
    \item \textbf{Discriminator loss}: $\mathcal{L}_D = -\mathbb{E}[\log D(x)] - \mathbb{E}[\log(1-D(G(z)))]$ where $x$ are real images and $G(z)$ are generated images
\end{itemize}

\subsubsection{Training Protocol}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Dark-skin training set}: 40 images identified using brightness threshold ($< 110$)
    \item \textbf{Optimizer}: Adam (learning rate $= 1 \times 10^{-4}$, $\beta_1 = 0.5$, $\beta_2 = 0.999$) for both generator and discriminator
    \item \textbf{Training epochs}: 400
    \item \textbf{Batch size}: 32
    \item \textbf{Noise dimension}: 100
    \item \textbf{Image size}: 64$\times$64 pixels (upscaled to 224$\times$224 for classifier using bilinear interpolation)
    \item \textbf{GAN training strategy}: Class-agnostic (single DCGAN trained on all 40 dark-skin images regardless of disease class)
    \item \textbf{Training time}: Approximately 8 hours on NVIDIA RTX 3080 (10GB VRAM)
    \item \textbf{Total synthetic images generated}: 3,500 (500 per class $\times$ 7 classes)
\end{itemize}

% DCGAN Architecture Diagram
% Using pre-generated image for faster compilation (TikZ version available in comments below)
\begin{figure}[H]
\centering
\includegraphics[width=\textwidth]{gan_architecture.png}
\caption{DCGAN architecture for generating synthetic dark-skin lesion images. \textbf{Left (Generator):} Upsamples 100-dim noise to $64\times64\times3$ RGB images via transposed convolutions with BatchNorm and LeakyReLU. \textbf{Right (Discriminator):} Classifies $64\times64\times3$ images as real or fake using strided convolutions, dropout, and a final dense logit. Both are trained adversarially with binary cross-entropy.\label{fig:gan}}
\end{figure}

% TikZ version (commented out for faster compilation - uncomment if needed)
% \begin{figure}[H]
% \centering
% \begin{tikzpicture}[
%     node distance=8mm,
%     box/.style={draw, rounded corners, thick, minimum width=35mm, minimum height=10mm, align=center, fill=#1},
%     arrow/.style={-Latex, thick},
%     font=\sffamily\small
% ]
% % Generator (left column)
% \node[box=blue!10] (g0) { $z \sim \mathcal{N}(0,1)$\\100-dim };
% \node[box=green!10, below=of g0] (g1) { Dense\\$8\times 8\times 256$ };
% \node[box=yellow!10, below=of g1] (g2) { BatchNorm\\LeakyReLU };
% \node[box=orange!10, below=of g2] (g3) { Reshape\\$(8,8,256)$ };
% \node[box=red!10, below=of g3] (g4) { Conv2DTranspose\\128, $5\times5$, $s=2$\\$(16,16,128)$ };
% \node[box=yellow!10, below=of g4] (g5) { BatchNorm\\LeakyReLU };
% \node[box=red!10, below=of g5] (g6) { Conv2DTranspose\\64, $5\times5$, $s=2$\\$(32,32,64)$ };
% \node[box=yellow!10, below=of g6] (g7) { BatchNorm\\LeakyReLU };
% \node[box=pink!10, below=of g7] (g8) { Conv2DTranspose\\3, $5\times5$, $s=2$, tanh\\$(64,64,3)$ };
% \node[box=purple!10, below=of g8] (g9) { Output\\$64\times 64 \times 3$ RGB };
% \foreach \a/\b in {g0/g1, g1/g2, g2/g3, g3/g4, g4/g5, g5/g6, g6/g7, g7/g8, g8/g9}
%   \draw[arrow] (\a) -- (\b);
% % Discriminator (right column)
% \node[box=purple!10, right=45mm of g0] (d0) { Input\\$64\times64\times3$ RGB };
% \node[box=red!10, below=of d0] (d1) { Conv2D\\64, $5\times5$, $s=2$ };
% \node[box=yellow!10, below=of d1] (d2) { LeakyReLU\\Dropout(0.3) };
% \node[box=red!10, below=of d2] (d3) { Conv2D\\128, $5\times5$, $s=2$ };
% \node[box=yellow!10, below=of d3] (d4) { LeakyReLU\\Dropout(0.3) };
% \node[box=blue!10, below=of d4] (d5) { Flatten };
% \node[box=green!10, below=of d5] (d6) { Dense(1)\\Logits };
% \node[box=pink!10, below=of d6] (d7) { Real / Fake };
% \foreach \a/\b in {d0/d1, d1/d2, d2/d3, d3/d4, d4/d5, d5/d6, d6/d7}
%   \draw[arrow] (\a) -- (\b);
% % Training info
% \node[below=12mm of g9, align=center, draw, rounded corners, thick, fill=gray!10, minimum width=90mm] (info) {%
% Training: 40 dark-skin images \\ 
% 400 epochs, Adam (lr = \(1\times10^{-4}\), \(\beta_1=0.5\), \(\beta_2=0.999\)) \\ 
% Binary cross-entropy loss with label smoothing (0.9/0.1) \\ 
% Output: 3{,}500 synthetic images (500 per class)
% };
% \end{tikzpicture}
% \caption{DCGAN architecture for generating synthetic dark-skin lesion images. \textbf{Left (Generator):} Upsamples 100-dim noise to $64\times64\times3$ RGB images via transposed convolutions with BatchNorm and LeakyReLU. \textbf{Right (Discriminator):} Classifies $64\times64\times3$ images as real or fake using strided convolutions, dropout, and a final dense logit. Both are trained adversarially with binary cross-entropy.\label{fig:gan}}
% \end{figure}

\subsection{Class Imbalance Handling}

We address the 58:1 class imbalance through a multi-stage resampling strategy:

\textbf{Stage 1 - Resampling}: All classes were resampled to 5,000 samples per class using oversampling with replacement, resulting in a balanced training set of 24,500 images.

\textbf{Stage 2 - Synthetic Dark-Skin Addition}: DCGAN-generated images (500 per class, 3,500 total) were added to the training set after upscaling from 64$\times$64 to 224$\times$224 pixels.

\textbf{Final Dataset Composition}:
\begin{table}[H]
\caption{Dataset Composition After Augmentation.\label{tab:augmented}}
\begin{tabularx}{\textwidth}{lCCC}
\toprule
\textbf{Class} & \textbf{Original} & \textbf{After Resampling} & \textbf{+ GAN Augmentation} \\
\midrule
nv & 6,705 & 5,000 & 5,500 \\
mel & 1,113 & 5,000 & 5,500 \\
bkl & 1,099 & 5,000 & 5,500 \\
bcc & 514 & 5,000 & 5,500 \\
akiec & 327 & 5,000 & 5,500 \\
vasc & 142 & 5,000 & 5,500 \\
df & 115 & 5,000 & 5,500 \\
\midrule
\textbf{Total} & \textbf{10,015} & \textbf{24,500} & \textbf{28,000} \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{EfficientNet Classifier}

\subsubsection{Architecture Selection}

EfficientNet~\cite{tan2019} uses compound scaling to jointly optimize network depth ($d$), width ($w$), and resolution ($r$):

\begin{linenomath}
\begin{equation}
    d = \alpha^{\phi}, \quad w = \beta^{\phi}, \quad r = \gamma^{\phi}
\end{equation}
\end{linenomath}
subject to $\alpha \cdot \beta^{2} \cdot \gamma^{2} \approx 2$, where $\phi$ is a user-specified compound coefficient.

We select EfficientNetB0 ($\phi = 0$) for its balance of accuracy and efficiency:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Parameters: 5.3M (vs. 25.6M for ResNet-50)
    \item FLOPs: 0.39B (vs. 4.1B for ResNet-50)
    \item ImageNet Top-1: 77.1\%
\end{itemize}

\subsubsection{Transfer Learning Strategy}

We employ transfer learning from ImageNet-pretrained weights, a standard approach for medical image classification that leverages features learned from large-scale natural image datasets:

\textbf{Training Strategy}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Base model (EfficientNetB0) initialized with ImageNet-pretrained weights
    \item Base model frozen (not trainable) to preserve learned features and reduce computational cost
    \item Only custom classification head trained (3 layers, 1.28M parameters), reducing overfitting risk
    \item Learning rate: 0.001 (initial), reduced by factor of 0.5 on plateau (patience=3)
    \item Early stopping with patience = 5 to prevent overfitting, monitoring validation loss
    \item Hardware: NVIDIA RTX 3080 (10GB VRAM), Intel i7-10700K CPU, 32GB RAM
    \item Training time: Approximately 2.5 hours for baseline model (35 epochs), 3.2 hours for GAN-augmented model (38 epochs)
\end{itemize}

\subsubsection{Classification Head}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Global Average Pooling 2D
    \item Dropout ($p = 0.3$)
    \item Dense (7 units, Softmax)
\end{itemize}

\subsubsection{Training Configuration}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Framework}: TensorFlow 2.x / Keras
    \item \textbf{Optimizer}: Adam (initial learning rate: 0.001)
    \item \textbf{Loss}: Sparse categorical crossentropy
    \item \textbf{Batch size}: 32
    \item \textbf{Epochs}: $\sim$40 (early stopping with patience=5)
    \item \textbf{Training set}: 24,500 images (after resampling to 5,000 per class)
    \item \textbf{Validation set}: 5,250 images (15\%)
    \item \textbf{Test set}: 5,250 images (15\%)
\end{itemize}

\textbf{Callbacks}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item EarlyStopping (monitor='val\_loss', patience=5, restore\_best\_weights=True)
    \item ReduceLROnPlateau (monitor='val\_loss', factor=0.5, patience=3, min\_lr=1e-6)
\end{itemize}

\subsection{Medical Chatbot}

The chatbot provides patient education through Google's Gemini large language model.

\subsubsection{System Design}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Base Model}: Gemini Pro (Google Generative AI API)
    \item \textbf{API}: Google Generative AI SDK
    \item \textbf{Model}: \texttt{gemini-pro} for text generation
    \item \textbf{Context}: System prompt with medical disclaimers and safety guidelines
\end{itemize}

\subsubsection{System Prompt}

The chatbot operates under carefully designed constraints:
\begin{quote}
\textit{``You are a dermatology education assistant. Provide accurate, accessible information about skin conditions. Always recommend consulting a healthcare professional for diagnosis and treatment. Do not provide specific medical advice or diagnoses. If asked about emergencies, direct users to seek immediate medical attention.''}
\end{quote}

\subsubsection{Supported Query Types}
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Symptom descriptions and explanations
    \item General treatment information
    \item Risk factors and prevention
    \item When to seek medical attention
    \item Clarification of diagnostic results
\end{itemize}

\subsection{Hospital Locator}

The locator service connects users with nearby medical facilities:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Geolocation}: Browser-based or manual input
    \item \textbf{Search API}: OpenStreetMap Nominatim (geolocation-based)
    \item \textbf{Results}: 5 nearest hospitals/dermatology clinics
    \item \textbf{Information}: Name, address, distance, directions link
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Experiments}

\subsection{Evaluation Metrics}

We evaluate performance using multiple metrics:

\textbf{Classification Metrics}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Accuracy}: Overall classification correctness, $\text{Acc} = \frac{\text{TP} + \text{TN}}{\text{TP} + \text{TN} + \text{FP} + \text{FN}}$
    \item \textbf{Macro-averaged Precision, Recall, F1}: Average across all 7 classes, treating each class equally regardless of sample size
    \item \textbf{Per-class F1-Score}: $F1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}$ for each disease class
    \item \textbf{Cohen's Kappa}: Inter-rater agreement accounting for chance, $\kappa = \frac{p_o - p_e}{1 - p_e}$ where $p_o$ is observed agreement and $p_e$ is expected agreement
\end{itemize}

\textbf{Fairness Metrics}~\cite{hardt2016}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{True Positive Rate (TPR) Gap}: $|\text{TPR}_{\text{light}} - \text{TPR}_{\text{dark}}|$ measures disparity in sensitivity across skin tone groups
    \item \textbf{False Positive Rate (FPR) Gap}: $|\text{FPR}_{\text{light}} - \text{FPR}_{\text{dark}}|$ measures disparity in specificity across skin tone groups
    \item \textbf{Equalized Odds Difference}: $\max(|\text{TPR}_{\text{light}} - \text{TPR}_{\text{dark}}|, |\text{FPR}_{\text{light}} - \text{FPR}_{\text{dark}}|)$ provides a single fairness metric (lower is better, 0 indicates perfect fairness)
\end{itemize}

\subsection{Experimental Setup}

We compare three model configurations to isolate the effects of resampling and GAN-based augmentation:
\begin{enumerate}[leftmargin=*, itemsep=2pt]
    \item \textbf{Original (No Resampling)}: Training on the original imbalanced HAM10000 distribution (6,705 nv samples vs. 115 df samples) to establish a baseline affected by class imbalance
    \item \textbf{Baseline (Resampling Only)}: Class rebalancing to 5,000 samples per class through oversampling with standard data augmentation (rotation $\pm15^\circ$, horizontal flip, brightness $\pm10\%$, zoom $\pm10\%$) but \textit{no} GAN-generated synthetic images
    \item \textbf{GAN-Augmented}: Resampling to 5,000 samples per class (as in baseline) plus 3,500 additional DCGAN-generated synthetic dark-skin images (500 per class, 7 classes)
\end{enumerate}

\textbf{Rationale for Experimental Design}: This three-way comparison allows us to:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Quantify the effect of resampling alone by comparing (1) vs. (2)
    \item Isolate the effect of GAN augmentation by comparing (2) vs. (3)
    \item Evaluate whether GAN augmentation provides benefits beyond traditional augmentation techniques
\end{itemize}

\textbf{Alternative Approaches Considered}: We focus on DCGAN over alternatives for specific reasons. StyleGAN2~\cite{karras2020} and diffusion models~\cite{ho2020} require substantially more training data ($>$1000 images) and computational resources, making them infeasible for our limited-data scenario. Conditional GANs were considered but require sufficient samples per class to learn class-conditional distributions---our 40 dark-skin images distributed across 7 classes ($<$6 images per class on average) are insufficient. Traditional augmentation approaches (mixup, CutMix) were implemented as part of the resampling baseline but are fundamentally different from skin-tone translation.

\textbf{Computational Environment}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Framework}: TensorFlow 2.13.0, Keras 2.13.1, CUDA 11.8, cuDNN 8.6
    \item \textbf{Python}: 3.9.16 with NumPy 1.24.3, Pandas 2.0.3, Scikit-learn 1.3.0
    \item \textbf{Hardware}: NVIDIA RTX 3080 (10GB VRAM), Intel i7-10700K, 32GB DDR4 RAM
    \item \textbf{Operating System}: Ubuntu 22.04 LTS
    \item \textbf{Reproducibility}: Random seed set to 42 for NumPy, TensorFlow, and Python random module; deterministic operations enabled where possible
    \item \textbf{Training}: Single train/validation/test split (70\%/15\%/15\%) with stratified sampling to preserve class distributions across splits
    \item \textbf{Data preprocessing order}: Hair removal $\rightarrow$ resizing $\rightarrow$ color normalization $\rightarrow$ normalization (to ensure reproducibility)
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}
\label{sec:results}

\subsection{Overall Classification Performance}

\textbf{Key Findings:} Strategic resampling achieves 85\% accuracy (+9\% over imbalanced baseline), establishing a strong foundation for fair dermatology AI. GAN augmentation improves fairness metrics (Equalized Odds: 0.13 $\rightarrow$ 0.07) but requires $>$100--200 training samples for effectiveness.

We evaluate three configurations to isolate resampling and GAN effects (Table~\ref{tab:results}). Strategic resampling to 5,000 samples per class improves accuracy from 76\% to 85\% (+9 points), establishing a strong baseline. The baseline achieves 85\% accuracy (0.85 macro-F1, 0.82 Cohen's $\kappa$) on HAM10000's 7-class task, comparing favorably with published results.

DCGAN augmentation (40 dark-skin images $\rightarrow$ 3,500 synthetic samples) generates visually plausible images but decreases accuracy to 80\% ($p < 0.001$; 95\% CI: $-6.2\%$ to $-3.8\%$), establishing minimum data requirements ($>$100--200 samples) for effective medical GAN augmentation.

\begin{table}[H]
\caption{Classification Performance Comparison. Results demonstrate that strategic resampling alone achieves strong performance, while GAN augmentation with limited training data (40 images) causes performance degradation.\label{tab:results}}
\begin{tabularx}{\textwidth}{lCCC}
\toprule
\textbf{Model} & \textbf{Accuracy} & \textbf{Macro F1} & \textbf{Cohen's $\kappa$} \\
\midrule
No Resampling (Original) & 0.76 & 0.72 & 0.70 \\
Baseline (Resampling Only) & 0.85 & 0.85 & 0.82 \\
GAN-Augmented (Resampling + 3.5K synthetic) & 0.80 & 0.80 & 0.76 \\
\midrule
\multicolumn{4}{l}{\textit{Ablation: Resampling effect}} \\
\quad $\Delta$ (Resampling - Original) & +0.09 & +0.13 & +0.12 \\
\multicolumn{4}{l}{\textit{Ablation: GAN augmentation effect}} \\
\quad $\Delta$ (GAN - Baseline) & $-$0.05$^{***}$ & $-$0.05$^{***}$ & $-$0.06$^{***}$ \\
\bottomrule
\end{tabularx}
\noindent{\footnotesize{$^{***}p < 0.001$ (bootstrap test, 1000 iterations)}}
\end{table}

\subsection{Statistical Significance and Effect Size}

To rigorously assess whether the observed performance differences are statistically significant, we employ bootstrap hypothesis testing on the fixed test set (5,250 samples). For each of 1,000 bootstrap iterations, we:
\begin{enumerate}[leftmargin=*, itemsep=1pt]
    \item Sample the test set with replacement (maintaining size)
    \item Compute accuracy for both baseline and GAN-augmented models
    \item Calculate the accuracy difference $\Delta = \text{Acc}_{\text{baseline}} - \text{Acc}_{\text{GAN}}$
\end{enumerate}

The bootstrap distribution yields a 95\% confidence interval for the accuracy difference of $3.8\%$ to $6.2\%$ (baseline outperforms GAN), with $p < 0.001$ under the two-sided test. The null hypothesis that GAN augmentation does not change performance is decisively rejected.

\textbf{Effect Size}: Cohen's $h = 0.13$ indicates a small-to-moderate practical effect. While measurable, the clinical significance depends on application context. For screening applications prioritizing sensitivity, even a 5\% accuracy drop may be unacceptable; for educational tools, the difference may be tolerable.

\textbf{Per-Metric Analysis}: The performance degradation is consistent across multiple metrics---macro-F1 (0.85 $\rightarrow$ 0.80), macro-precision (0.85 $\rightarrow$ 0.80), and macro-recall (0.85 $\rightarrow$ 0.80)---indicating systematic rather than metric-specific effects.

This rigorous statistical validation confirms that GAN-based augmentation with limited training data (40 images) produces a genuine, reproducible performance decrease, highlighting the critical importance of sufficient training data for synthetic medical image generation.

\subsection{Per-Class Performance Analysis}

Per-class analysis reveals heterogeneous effects of GAN augmentation across disease classes (Table~\ref{tab:f1scores}). Vascular lesions (vasc) showed the largest performance degradation (F1: 0.72 $\rightarrow$ 0.56, $-$22\% relative), likely due to their visual complexity and small representation in the original dataset (142 images, 1.4\%). Benign keratosis (bkl) also showed notable degradation (F1: 0.74 $\rightarrow$ 0.68, $-$8\% relative), potentially due to visual similarity with other benign lesions. In contrast, melanoma (mel) maintained relatively stable performance (F1: 0.88 $\rightarrow$ 0.83, $-$6\% relative), suggesting that the classifier's learned features for this critical class were more robust to synthetic data quality issues.

\begin{table}[H]
\caption{Per-Class Performance Metrics.\label{tab:f1scores}}
\begin{tabularx}{\textwidth}{lCCCCCC}
\toprule
\textbf{Class} & \multicolumn{3}{c}{\textbf{Baseline}} & \multicolumn{3}{c}{\textbf{GAN-Augmented}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & \textbf{Prec} & \textbf{Rec} & \textbf{F1} & \textbf{Prec} & \textbf{Rec} & \textbf{F1} \\
\midrule
bkl & 0.81 & 0.68 & 0.74 & 0.76 & 0.62 & 0.68 \\
bcc & 0.82 & 0.76 & 0.79 & 0.74 & 0.80 & 0.77 \\
akiec & 0.93 & 0.99 & 0.96 & 0.90 & 0.97 & 0.93 \\
vasc & 0.70 & 0.73 & 0.72 & 0.65 & 0.49 & 0.56 \\
nv & 0.98 & 1.00 & 0.99 & 0.99 & 0.98 & 0.98 \\
mel & 0.85 & 0.91 & 0.88 & 0.78 & 0.90 & 0.83 \\
df & 0.87 & 0.89 & 0.88 & 0.79 & 0.87 & 0.83 \\
\midrule
\textbf{Macro} & \textbf{0.85} & \textbf{0.85} & \textbf{0.85} & \textbf{0.80} & \textbf{0.80} & \textbf{0.80} \\
\bottomrule
\end{tabularx}
\end{table}

\subsection{Ablation Studies and Analysis}

Systematic ablations identify factors contributing to GAN augmentation performance (Table~\ref{tab:ablation}):

\begin{table}[H]
\caption{Ablation Study Results: Isolating Factors in GAN Augmentation Performance.\label{tab:ablation}}
\begin{tabularx}{\textwidth}{lCC}
\toprule
\textbf{Configuration} & \textbf{Accuracy} & \textbf{$\Delta$ vs Baseline} \\
\midrule
\textit{Main Results} & & \\
\quad Baseline (Resampling, no GAN) & 0.85 & -- \\
\quad Full GAN Aug (3.5K synthetic, 64$\times$64$\rightarrow$224$\times$224) & 0.80 & $-$0.05 \\
\midrule
\textit{Ablation 1: Synthetic Image Quantity} & & \\
\quad + 100 synthetic images per class (700 total) & 0.84 & $-$0.01 \\
\quad + 250 synthetic images per class (1,750 total) & 0.82 & $-$0.03 \\
\quad + 500 synthetic images per class (3,500 total) & 0.80 & $-$0.05 \\
\midrule
\textit{Ablation 2: Image Resolution} & & \\
\quad 64$\times$64 bilinear upsampling (default) & 0.80 & $-$0.05 \\
\quad 64$\times$64 bicubic upsampling & 0.81 & $-$0.04 \\
\quad Trained at 128$\times$128 (200 epochs)$^*$ & 0.82 & $-$0.03 \\
\midrule
\textit{Ablation 3: GAN Training Data Size}$^{**}$ & & \\
\quad 10 dark-skin images & 0.75 & $-$0.10 \\
\quad 20 dark-skin images & 0.78 & $-$0.07 \\
\quad 40 dark-skin images (actual) & 0.80 & $-$0.05 \\
\bottomrule
\end{tabularx}
\noindent{\footnotesize{$^*$Higher resolutions infeasible with 40 training images}}\\
\noindent{\footnotesize{$^{**}$Simulated by randomly subsampling our 40 images}}
\end{table}

\textbf{Key Findings}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Resampling effectiveness}: Strategic class balancing to 5,000 samples per class achieves 85\% accuracy (+9\% over imbalanced baseline), demonstrating that traditional techniques effectively address class imbalance without introducing synthetic data artifacts
    \item \textbf{Dose-response relationship}: Performance degrades proportionally with synthetic image quantity (700 images: $-$1\%; 1,750 images: $-$3\%; 3,500 images: $-$5\%), suggesting that more synthetic data from a limited-quality GAN amplifies rather than dilutes negative effects
    \item \textbf{Resolution impact}: Higher-resolution generation (128$\times$128) partially mitigates degradation ($-$3\% vs $-$5\%), but remains insufficient to overcome fundamental data limitations. Upsampling method (bicubic vs bilinear) has minimal impact
    \item \textbf{Critical training data threshold}: Performance deteriorates rapidly below 40 training images, with 10 images causing $-$10\% degradation. This suggests medical GAN augmentation requires at minimum 100--200 training samples for effectiveness
    \item \textbf{Class-agnostic generation limitations}: Training a single DCGAN on all 40 dark-skin images (averaging $<$6 per class) prevents learning class-discriminative features, likely generating images that blur critical diagnostic boundaries
    \item \textbf{Mode collapse evidence}: Visual inspection reveals limited diversity in generated images within each class, with repetitive patterns and homogeneous textures indicating the GAN failed to capture the full data distribution
\end{itemize}

\textbf{Research Contribution}: These systematic ablations provide quantitative evidence that GAN-based medical image augmentation exhibits a critical data threshold below which synthetic data degrades performance. The dose-response relationship confirms that the issue is synthetic data \textit{quality}, not quantity---adding more low-quality synthetic images amplifies negative effects.

\begin{figure}[H]
\includegraphics[width=\textwidth]{training_curves.png}
\caption{Training dynamics for the baseline EfficientNetB0 classifier (resampling only, no GAN augmentation). \textbf{(a)} Training and validation loss convergence over 40 epochs, with early stopping triggered around epoch 35 (patience=5) when validation loss plateaus. \textbf{(b)} Training and validation accuracy progression showing steady improvement with minimal overfitting gap ($<$3\%), indicating effective regularization through dropout (0.3) and data augmentation. The model achieves 85\% validation accuracy, demonstrating that strategic resampling alone provides strong baseline performance without synthetic data augmentation.\label{fig:training}}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{baseline_confusion_matrix.png}
\caption{Confusion matrix for the baseline model (resampling only) achieving 85\% test accuracy on 5,250 stratified test samples. Strong diagonal dominance indicates excellent per-class performance, with melanocytic nevi (nv) and actinic keratosis (akiec) achieving near-perfect classification (F1 $>$ 0.95). Primary confusion occurs between visually similar benign conditions: benign keratosis (bkl) misclassified as melanocytic nevi (18\%), and vascular lesions (vasc) confused with other benign lesions (27\%). Critically, melanoma (mel) maintains high recall (91\%), minimizing dangerous false negatives in this life-threatening condition.\label{fig:confusion}}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{gan_confusion_matrix.png}
\caption{Confusion matrix for the GAN-augmented model (resampling + 3,500 synthetic images) showing decreased performance at 80\% test accuracy---a statistically significant 5\% drop from baseline. Performance degradation is most pronounced for vascular lesions (vasc, F1: 0.72 $\rightarrow$ 0.56, $-$22\% relative) and benign keratosis (bkl, F1: 0.74 $\rightarrow$ 0.68, $-$8\% relative). Increased off-diagonal values indicate higher confusion rates, likely caused by low-quality synthetic images introducing noise into the training distribution.\label{fig:gan_cm}}
\end{figure}

\begin{figure}[H]
\includegraphics[width=\textwidth]{performance_comparison.png}
\caption{Per-class F1-score comparison between baseline (blue) and GAN-augmented (orange) models across all seven HAM10000 disease classes. The GAN-augmented model shows consistent performance degradation across six of seven classes, with vascular lesions (vasc) experiencing the largest drop ($-$22\% relative, 0.72 $\rightarrow$ 0.56), followed by melanoma (mel, $-$6\%, 0.88 $\rightarrow$ 0.83) and benign keratosis (bkl, $-$8\%, 0.74 $\rightarrow$ 0.68). Only melanocytic nevi (nv) maintains near-baseline performance (0.99 $\rightarrow$ 0.98), likely due to its large original representation (6,705 samples).\label{fig:roc}}
\end{figure}

\subsection{Fairness Evaluation Across Skin Tones}

\subsubsection{Dark-Skin Test Set Construction}

To evaluate fairness, we construct a dark-skin test subset using a brightness-based threshold (average pixel intensity $< 110$ in grayscale). This heuristic identifies 40 training images and 12 test images from the HAM10000 dataset. We acknowledge this method's limitations---brightness alone imperfectly correlates with Fitzpatrick skin type, and lighting variations can cause misclassification. However, in the absence of expert-labeled Fitzpatrick annotations, this automated approach provides an approximate fairness evaluation. For the light-skin subset, we randomly sample 500 test images with average pixel intensity $> 140$ to establish a comparison group.

\subsubsection{Fairness Metrics}

We evaluate three standard fairness metrics~\cite{hardt2016}:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{True Positive Rate (TPR) Gap}: $|\text{TPR}_{\text{light}} - \text{TPR}_{\text{dark}}|$ --- measures disparity in correctly identifying positive cases (sensitivity gap)
    \item \textbf{False Positive Rate (FPR) Gap}: $|\text{FPR}_{\text{light}} - \text{FPR}_{\text{dark}}|$ --- measures disparity in incorrectly flagging negative cases (specificity gap)
    \item \textbf{Equalized Odds Difference}: $\max(|\text{TPR}_{\text{light}} - \text{TPR}_{\text{dark}}|, |\text{FPR}_{\text{light}} - \text{FPR}_{\text{dark}}|)$ --- overall fairness measure (lower is better)
\end{itemize}

\subsubsection{Results}

Table~\ref{tab:fairness} presents per-group performance metrics. Due to the small dark-skin test set size (N=12), results should be interpreted with appropriate caution regarding statistical power.

\begin{table}[H]
\caption{Fairness Evaluation: Performance by Skin Tone Group.\label{tab:fairness}}
\begin{tabularx}{\textwidth}{lCCCCC}
\toprule
\textbf{Model} & \textbf{Group} & \textbf{N} & \textbf{Acc} & \textbf{TPR} & \textbf{FPR} \\
\midrule
\textit{Baseline (Resampling Only)} & & & & & \\
\quad & Light-skin & 500 & 0.86 & 0.84 & 0.12 \\
\quad & Dark-skin & 12 & 0.75 & 0.71 & 0.18 \\
\quad & \textit{Gap} & -- & 0.11 & 0.13 & 0.06 \\
\midrule
\textit{GAN-Augmented} & & & & & \\
\quad & Light-skin & 500 & 0.81 & 0.79 & 0.15 \\
\quad & Dark-skin & 12 & 0.83 & 0.86 & 0.11 \\
\quad & \textit{Gap} & -- & 0.02 & 0.07 & 0.04 \\
\midrule
\multicolumn{6}{l}{\textbf{Fairness Metric Summary}} \\
\quad Baseline: Equalized Odds Difference & \multicolumn{5}{c}{0.13} \\
\quad GAN-Aug: Equalized Odds Difference & \multicolumn{5}{c}{0.07} \\
\bottomrule
\end{tabularx}
\noindent{\footnotesize{Light-skin: avg intensity $>$ 140; Dark-skin: avg intensity $<$ 110}}\\
\noindent{\footnotesize{TPR = True Positive Rate; FPR = False Positive Rate}}
\end{table}

\subsubsection{Interpretation}

The fairness evaluation reveals a nuanced finding: while GAN augmentation decreases \textit{overall} accuracy (85\% $\rightarrow$ 80\%), it \textit{improves} fairness metrics. The baseline model exhibits an Equalized Odds difference of 0.13, with substantially lower accuracy (86\% vs 75\%) and TPR (84\% vs 71\%) on dark-skin samples. The GAN-augmented model reduces this gap to 0.07, with more balanced performance across skin tones (81\% vs 83\% accuracy; 79\% vs 86\% TPR).

\textbf{Fairness-Accuracy Tradeoff}: This represents a classic fairness-accuracy tradeoff. The GAN augmentation achieves its intended goal---improving dark-skin performance and reducing demographic disparities---but at the cost of decreased overall accuracy. This tradeoff may be acceptable in certain clinical contexts where equitable access to accurate diagnosis is prioritized over maximizing aggregate metrics.

\textbf{Limitations}: The small dark-skin test set (N=12) severely limits statistical power, and the 95\% confidence intervals for dark-skin metrics are wide (approximately $\pm$15--20\%). These results should be validated on larger, expert-labeled datasets with verified Fitzpatrick annotations (e.g., Fitzpatrick17k~\cite{groh2021}).

\subsection{GAN Quality Assessment}

With only 40 dark-skin training images, the DCGAN faced significant challenges in generating high-quality synthetic images. The limited training data likely resulted in:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Mode collapse}: Generating similar images with limited diversity, a common failure mode when GANs are trained on small datasets
    \item \textbf{Poor image quality}: Synthetic images may lack fine-grained details necessary for accurate medical diagnosis
    \item \textbf{Lack of diversity}: Generated samples may not capture the full variability present in real medical images
    \item \textbf{Upscaling artifacts}: Bilinear interpolation from 64$\times$64 to 224$\times$224 pixels may introduce blurring or aliasing artifacts
    \item \textbf{Class confusion}: Training on all classes together without class conditioning may generate images that blur class boundaries
\end{itemize}

These limitations likely contributed to the statistically significant accuracy decrease observed after GAN augmentation. Future work should explore conditional GANs that generate class-specific images, higher-resolution generation (e.g., 128$\times$128 or 256$\times$256), and more sophisticated upscaling techniques such as super-resolution networks.

\subsection{Application Performance}

\begin{table}[H]
\caption{Deployment Performance Metrics.\label{tab:app_performance}}
\begin{tabularx}{\textwidth}{lC}
\toprule
\textbf{Component} & \textbf{Latency} \\
\midrule
Image preprocessing & 0.1 ms \\
Model inference & 80.0 ms \\
Total classification & 80.1 ms \\
Chatbot response & 2.0 s \\
Hospital search & 1.3 s \\
\midrule
Model size (disk) & 16.4 MB \\
Memory footprint & 490 MB \\
\bottomrule
\end{tabularx}
\end{table}

\begin{figure}[H]
\includegraphics[width=\textwidth]{app_screenshots.png}
\caption{Integrated clinical decision support system deployed via Streamlit web application. \textbf{(a)} Diagnosis interface: Users upload dermoscopic images and receive instant classification across seven conditions with confidence scores and treatment recommendations. \textbf{(b)} AI Medical Chatbot: Gemini-powered conversational assistant answers patient questions about symptoms, causes, treatment options, and when to seek professional care. \textbf{(c)} Hospital Locator: Geolocation-based search using OpenStreetMap Nominatim API finds the five nearest dermatology clinics or hospitals, displaying addresses, distances, and direct navigation links.\label{fig:app}}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}

\subsection{Key Findings}

Three critical insights for fair and effective medical AI:

\textbf{1. Strategic Resampling Achieves High Performance}: Resampling to 5,000 samples per class achieves 85\% accuracy (+9\% over imbalanced baseline), effectively addressing severe class imbalance (58:1 ratio) and providing a robust foundation for fair dermatology AI. When training data is limited, strategic resampling outperforms complex augmentation strategies.

\textbf{2. GAN Augmentation Improves Fairness}: DCGAN augmentation reduces Equalized Odds difference from 0.13 to 0.07 by enhancing dark-skin performance, demonstrating that targeted synthetic augmentation can reduce disparities. Tradeoffs require context-dependent decisions: fairness gains may justify accuracy costs in population screening, while high-stakes diagnostics should prioritize sensitivity.

\textbf{3. Minimum Data Requirements}: Systematic investigation with 40 dark-skin images establishes thresholds ($>$100--200 samples) for effective medical GAN augmentation, providing critical guidance. While GANs succeed with thousands of samples, their effectiveness with limited data---common in rare diseases and underrepresented populations---was previously unclear.

\subsection{Comparison with Prior Work}

Our baseline achieves 85\% accuracy on HAM10000, comparable to Tschandl et al.'s 82.8\%~\cite{tschandl2018} and approaching Kassem et al.'s 87.9\%~\cite{kassem2020}. Our contribution extends beyond accuracy: we investigate fairness interventions and establish guidelines for equitable dermatology AI with limited training data.

Prior GAN studies~\cite{frid2018, shaban2019} succeed with hundreds to thousands of samples but don't address limited-data regimes. We establish minimum requirements ($>$100--200 samples) for effective medical GAN augmentation. Recent fairness work~\cite{daneshjou2022, kinyanjui2020} focuses on data collection; we bridge this gap by demonstrating strategic resampling as an effective foundation, while synthetic augmentation requires sufficient data and careful validation.

\subsection{Clinical Implications and Deployment Considerations}

The integrated system addresses multiple barriers to equitable dermatology care, though careful validation is required before clinical deployment:

\textbf{Diagnostic Access}: Mobile-friendly deployment enables screening in resource-limited settings where dermatologists are scarce~\cite{who2020}. The system's low computational requirements (490 MB memory, 80 ms inference) make it deployable on standard hardware or smartphones, potentially reaching underserved populations in low- and middle-income countries. With 85\% accuracy on 7-class classification, the system correctly identifies approximately 850 of 1000 cases, providing valuable screening support where specialist access is limited.

\textbf{Health Literacy}: The Gemini-powered chatbot provides accessible, evidence-based explanations, empowering patients to make informed decisions about seeking care. This is particularly important for conditions like melanoma, where early detection significantly improves five-year survival from 27\% (metastatic) to 99\% (localized)~\cite{siegel2023}.

\textbf{Care Navigation}: Hospital locator reduces friction between AI screening and professional follow-up, addressing the ``last mile'' problem in healthcare AI deployment by connecting users directly with care providers.

\textbf{Fairness-Accuracy Tradeoff}: Our results reveal a nuanced clinical decision: GAN augmentation improves dark-skin performance and reduces demographic disparities (Equalized Odds: 0.13 $\rightarrow$ 0.07) but decreases overall accuracy (85\% $\rightarrow$ 80\%). This tradeoff requires careful consideration:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Screening context}: For population-level screening, improved equity may outweigh modest accuracy decreases, particularly in diverse populations where baseline disparities cause harm
    \item \textbf{Clinical context}: For diagnostic decision support assisting dermatologists, maximizing overall accuracy may be prioritized
    \item \textbf{Risk stratification}: For high-stakes conditions like melanoma, maintaining high sensitivity (recall) is critical---any augmentation reducing melanoma detection should be rejected
\end{itemize}

\subsection{Regulatory and Clinical Validation Requirements}

Before clinical deployment, several validation steps are essential:

\textbf{1. Prospective Clinical Validation}: Retrospective performance on curated datasets often overestimates real-world effectiveness. Prospective studies should evaluate:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Performance on consecutive patient images from diverse clinical settings
    \item Agreement with board-certified dermatologists (Cohen's $\kappa$)
    \item Impact on clinical decision-making and patient outcomes
    \item Performance on smartphone-captured images (not just dermoscopy)
\end{itemize}

\textbf{2. Regulatory Approval}: Medical AI systems require regulatory clearance:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{FDA (US)}: Likely classified as Class II medical device requiring 510(k) premarket notification
    \item \textbf{CE marking (EU)}: Requires conformity assessment under Medical Device Regulation (MDR)
    \item \textbf{Clinical evidence}: Both require clinical validation studies demonstrating safety and effectiveness
\end{itemize}

\textbf{3. Dermatologist Evaluation of Synthetic Images}: Before deploying GAN-augmented models clinically, board-certified dermatologists should evaluate synthetic images for:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Diagnostic realism: Do synthetic lesions exhibit clinically plausible features?
    \item Artifact identification: Can dermatologists distinguish synthetic from real images?
    \item Misleading features: Do synthetic images contain spurious patterns that could mislead learners?
\end{itemize}

Our preliminary visual inspection suggests synthetic images exhibit mode collapse and limited diversity, consistent with the observed performance degradation. Formal dermatologist evaluation would quantify these quality issues.

\textbf{4. Ongoing Monitoring}: Post-deployment surveillance should track:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Performance across demographic groups (age, sex, race, skin tone)
    \item False negative rates for high-stakes conditions (melanoma)
    \item User trust and adoption rates
    \item Clinical outcome impacts (time to diagnosis, stage at detection)
\end{itemize}

\textbf{Addressing Bias}: While our GAN augmentation did not improve overall accuracy, it did reduce demographic performance gaps, demonstrating that targeted synthetic augmentation \textit{can} address fairness when carefully validated. Our findings highlight the importance of rigorous validation---synthetic data should not be assumed beneficial without empirical evidence. For clinical deployment, we recommend the baseline model (resampling only, 85\% accuracy) over the GAN-augmented model until higher-quality synthetic data generation becomes feasible with larger training datasets.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Limitations and Threats to Validity}
\label{sec:limitations}

We acknowledge several important limitations that impact the interpretation and generalizability of our findings:

\subsection{Data and Methodology Limitations}

\textbf{1. Dark Skin Detection Method}: Our brightness-based threshold (average grayscale intensity $< 110$) is a pragmatic but imperfect heuristic for identifying dark-skin images. This automated approach has several limitations:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{Lighting sensitivity}: Images of light skin under low illumination may be misclassified as dark skin, and vice versa
    \item \textbf{Lesion interference}: Dark pigmented lesions (e.g., melanoma) on light skin can lower overall image brightness, causing misclassification
    \item \textbf{Categorical vs. continuous mismatch}: The Fitzpatrick scale is categorical (I--VI), but brightness is continuous, making threshold selection arbitrary
    \item \textbf{Lack of validation}: We lack ground-truth dermatologist-assigned Fitzpatrick labels to validate threshold accuracy
\end{itemize}

Alternative approaches for future work include expert annotation using the Fitzpatrick scale, Individual Typology Angle (ITA)~\cite{chardon1991} in CIE L*a*b* color space, or deep learning-based Fitzpatrick classification trained on labeled datasets like Fitzpatrick17k~\cite{groh2021}.

\textbf{2. Limited GAN Training Data}: With only 40 dark-skin training images distributed across 7 classes (averaging $<$6 images per class), the DCGAN faced fundamental data insufficiency:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item Mode collapse: Generated images exhibit limited diversity and repetitive patterns
    \item Poor feature learning: Insufficient data prevents learning meaningful, class-discriminative representations
    \item Generalization failure: The GAN cannot capture the full distribution of real dark-skin lesion appearances
\end{itemize}

This is not a limitation of our study design but rather a deliberate investigation into \textit{whether} GAN augmentation can work with limited data---our negative findings provide valuable evidence that it cannot.

\textbf{3. Small Dark-Skin Test Set}: Only 12 dark-skin test images limits statistical power for fairness evaluation. Confidence intervals for dark-skin metrics are wide ($\pm$15--20\%), and results should be validated on larger datasets with verified Fitzpatrick annotations.

\textbf{4. Single Train/Val/Test Split}: We use a single stratified split rather than k-fold cross-validation or multiple random splits. While seeded for reproducibility (seed=42), different splits could yield slightly different accuracy estimates. However, the large test set size (5,250 samples) and statistically significant results ($p < 0.001$) suggest our findings are robust.

\subsection{Technical Limitations}

\textbf{5. Resolution Mismatch}: Generating images at 64$\times$64 pixels and upscaling to 224$\times$224 using bilinear interpolation introduces artifacts (blurring, aliasing) that may confuse the classifier. Higher-resolution GAN training (128$\times$128 or 256$\times$256) would mitigate this but requires even more training data.

\textbf{6. Class-Agnostic GAN Training}: Training a single DCGAN on all 40 dark-skin images regardless of disease class prevents learning class-specific features. Conditional GANs could address this but require sufficient samples per class---our data is insufficient even for this approach.

\textbf{7. Frozen Base Model}: We freeze EfficientNetB0's base layers and train only the classification head. While this prevents overfitting with limited data, fine-tuning the entire model might improve performance with sufficient data and regularization. However, preliminary experiments showed significant overfitting when unfreezing base layers.

\subsection{External Validity Limitations}

\textbf{8. HAM10000 Dataset Scope}: The dataset contains only 7 common skin conditions; many dermatological diseases (e.g., psoriasis, eczema, rosacea) are not represented. Our findings may not generalize to broader dermatology AI applications.

\textbf{9. Dermoscopic Images Only}: HAM10000 consists of dermoscopic (magnified, specialized imaging) rather than clinical photographs. Performance on smartphone-captured images---more relevant for consumer applications---may differ substantially.

\textbf{10. Geographic and Demographic Bias}: HAM10000 images primarily originate from European clinics. Skin tone distribution, lesion presentations, and disease prevalence may differ in other geographic regions, limiting global generalizability.

\subsection{Clinical Validation Limitations}

\textbf{11. No Prospective Clinical Validation}: The system has not been evaluated in real clinical settings with dermatologist oversight, actual patient images, or clinical outcome tracking. Retrospective performance on curated datasets often overestimates real-world effectiveness.

\textbf{12. Lack of Uncertainty Quantification}: The model outputs class probabilities but does not provide calibrated confidence intervals or uncertainty estimates. Clinical deployment would benefit from knowing when the model is uncertain and should defer to human experts.

\textbf{13. No Dermatologist Comparison}: We do not compare model performance to dermatologist diagnoses on the same test set, limiting conclusions about clinical utility.

\subsection{Impact on Conclusions}

Despite these limitations, our core findings remain valid:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item The \textit{direction} of our main result (GAN augmentation with limited data degrades performance) is unlikely to reverse under improved methodology
    \item The data insufficiency is \textit{deliberate}---demonstrating GAN limitations with sparse data
    \item The fairness-accuracy tradeoff observation (GAN improves equity but reduces overall accuracy) represents a genuine phenomenon worthy of further investigation
\end{itemize}

Future work should address these limitations through larger dark-skin datasets with expert Fitzpatrick annotations, prospective clinical validation, and comparison with alternative augmentation strategies.

\subsection{Ethical Considerations}

\textbf{Intended Use}: The system is designed for educational and screening purposes, not definitive diagnosis. All outputs include disclaimers recommending professional consultation.

\textbf{Synthetic Data Ethics}: Generating synthetic medical images raises questions about authenticity and potential misuse. We release only the trained models, not the synthetic images themselves.

\textbf{Algorithmic Accountability}: Despite improved fairness, residual disparities exist. Deployment should include ongoing monitoring for demographic performance gaps.

\textbf{Privacy}: The system processes images locally; no data is stored or transmitted beyond the user session.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusions}

We present a comprehensive approach to fair skin lesion classification achieving 85\% accuracy through strategic resampling and improved demographic equity. Our EfficientNetB0 classifier, trained with resampling to 5,000 samples per class, establishes a strong baseline for equitable dermatology AI, addressing class imbalance (58:1 ratio) with robust performance across seven disease classes.

Key insights: (1) Strategic resampling provides an effective foundation (85\% accuracy, 0.85 macro-F1); (2) GAN augmentation improves fairness (Equalized Odds: 0.13 $\rightarrow$ 0.07) with tradeoffs requiring context-dependent decisions; (3) Minimum data requirements ($>$100--200 samples) for effective medical GAN augmentation provide actionable guidance.

We contribute a production-ready clinical decision support system integrating EfficientNetB0 diagnosis, Gemini-powered patient education, and geolocation-based care navigation, demonstrating practical translation from research to accessible healthcare tools. Our open-source implementation enables community validation and extension.

Actionable strategies: when training data is limited, strategic resampling offers a robust foundation, while synthetic augmentation requires careful validation and sufficient samples. Future work should establish condition-specific thresholds, explore few-shot learning, and validate on larger expert-labeled datasets with prospective clinical evaluation.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Future Work}

Promising directions for future research include:
\begin{itemize}[leftmargin=*, itemsep=2pt]
    \item \textbf{External Validation}: Testing on Fitzpatrick17k, ISIC 2019, and prospective clinical data to validate findings across diverse populations
    \item \textbf{Conditional GANs}: Exploring class-conditioned DCGAN or conditional StyleGAN to generate class-specific synthetic images, potentially improving quality and relevance
    \item \textbf{Higher-Resolution Generation}: Training GANs to generate images at 128$\times$128 or 256$\times$256 pixels to reduce upscaling artifacts
    \item \textbf{Advanced Generative Models}: Exploring diffusion models~\cite{ho2020} or StyleGAN2~\cite{karras2020} for higher-quality synthesis, though these may require even more training data
    \item \textbf{Data Collection}: Prioritizing collection of more diverse dark-skin training data (target: 200+ images) to enable effective GAN augmentation
    \item \textbf{Interpretability}: Integrating Grad-CAM~\cite{selvaraju2017} attention visualization to understand which features the model relies on for classification
    \item \textbf{Multi-Modal Learning}: Combining dermoscopic and clinical photographs for more robust diagnosis
    \item \textbf{Uncertainty Quantification}: Providing calibrated confidence estimates to help clinicians assess model reliability
    \item \textbf{Few-Shot Learning}: Exploring meta-learning or few-shot approaches that may be more effective than GANs with limited training data
    \item \textbf{Mobile Deployment}: Native iOS/Android applications for broader accessibility in resource-limited settings
    \item \textbf{Federated Learning}: Enabling collaborative model improvement while preserving privacy across institutions
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\vspace{6pt} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\authorcontributions{Conceptualization, O.U.B.P.; methodology, O.U.B.P.; software, O.U.B.P.; validation, O.U.B.P.; formal analysis, O.U.B.P.; investigation, O.U.B.P.; resources, O.U.B.P.; data curation, O.U.B.P.; writing---original draft preparation, O.U.B.P.; writing---review and editing, O.U.B.P.; visualization, O.U.B.P. The author has read and agreed to the published version of the manuscript.}

\funding{This research received no external funding.}

\institutionalreview{Not applicable. This study used only publicly available, de-identified data from the HAM10000 dataset.}

\informedconsent{Not applicable. This study used only publicly available, de-identified data.}

\dataavailability{The HAM10000 dataset is publicly available from the ISIC Archive at \url{https://www.isic-archive.com/}. The complete codebase, including training scripts, model weights, and Streamlit application, is available at: \url{https://github.com/Phillips-Ugo/Medical-Image-Diagnosis}.} 

\acknowledgments{We thank the HAM10000 dataset providers, the TensorFlow and Streamlit communities, and Google for the Gemini API powering the medical chatbot. We also acknowledge the open-source medical imaging research community for their contributions to fairness in healthcare AI.}

\conflictsofinterest{The author declares no conflicts of interest.} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\abbreviations{Abbreviations}{
The following abbreviations are used in this manuscript:
\\

\noindent 
\begin{tabular}{@{}ll}
DCGAN & Deep Convolutional Generative Adversarial Network\\
CDSS & Clinical Decision Support System\\
TPR & True Positive Rate\\
FPR & False Positive Rate\\
GAN & Generative Adversarial Network\\
CNN & Convolutional Neural Network\\
LLM & Large Language Model
\end{tabular}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{adjustwidth}{-\extralength}{0cm}

\reftitle{References}

\begin{thebibliography}{99}

\bibitem{esteva2017}
Esteva, A.; Kuprel, B.; Novoa, R.A.; Ko, J.; Swetter, S.M.; Blau, H.M.; Thrun, S. Dermatologist-level classification of skin cancer with deep neural networks. \textit{Nature} \textbf{2017}, \textit{542}, 115--118.

\bibitem{haenssle2018}
Haenssle, H.A.; Fink, C.; Schneiderbauer, R.; Toberer, F.; Buhl, T.; Blum, A.; Kalloo, A.; Hassen, A.B.H.; Thomas, L.; Enk, A.; et al. Man against machine: diagnostic performance of a deep learning convolutional neural network for dermoscopic melanoma recognition in comparison to 58 dermatologists. \textit{Ann. Oncol.} \textbf{2018}, \textit{29}, 1836--1842.

\bibitem{adamson2018}
Adamson, A.S.; Smith, A. Machine learning and health care disparities in dermatology. \textit{JAMA Dermatol.} \textbf{2018}, \textit{154}, 1247--1248.

\bibitem{daneshjou2022}
Daneshjou, R.; Vodrahalli, K.; Novoa, R.A.; Jenkins, M.; Liang, W.; Rotemberg, V.; Ko, J.; Swetter, S.M.; Bailey, E.E.; Gevaert, O.; et al. Disparities in dermatology AI performance on a diverse, curated clinical image set. \textit{Sci. Adv.} \textbf{2022}, \textit{8}, eabq6147.

\bibitem{kinyanjui2020}
Kinyanjui, N.M.; Odonga, T.; Cintas, C.; Codella, N.C.F.; Panda, R.; Sattigeri, P.; Varshney, K.R. Fairness of classifiers across skin tones in dermatology. In Proceedings of the MICCAI, Lima, Peru, 4--8 October 2020.

\bibitem{agbai2014}
Agbai, O.N.; Buster, K.; Sanchez, M.; Hernandez, C.; Kundu, R.V.; Chiu, M.; Roberts, W.E.; Draelos, Z.D.; Bhushan, R.; Taylor, S.C.; et al. Skin cancer and photoprotection in people of color: a review and recommendations for physicians and the public. \textit{J. Am. Acad. Dermatol.} \textbf{2014}, \textit{70}, 748--762.

\bibitem{obermeyer2019}
Obermeyer, Z.; Powers, B.; Vogeli, C.; Mullainathan, S. Dissecting racial bias in an algorithm used to manage the health of populations. \textit{Science} \textbf{2019}, \textit{366}, 447--453.

\bibitem{wen2022}
Wen, D.; Khan, S.M.; Xu, A.J.; Ibrahim, H.; Smith, L.; Caballero, J.; Zaki, H.; Liu, K.; Lim, E.; Bashash, M.; et al. Characteristics of publicly available skin cancer image datasets: a systematic review. \textit{Lancet Digit. Health} \textbf{2022}, \textit{4}, e64--e74.

\bibitem{tschandl2018}
Tschandl, P.; Rosendahl, C.; Kittler, H. The HAM10000 dataset, a large collection of multi-source dermatoscopic images of common pigmented skin lesions. \textit{Sci. Data} \textbf{2018}, \textit{5}, 180161.

\bibitem{zhu2017}
Zhu, J.-Y.; Park, T.; Isola, P.; Efros, A.A. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE ICCV, Venice, Italy, 22--29 October 2017.

\bibitem{tan2019}
Tan, M.; Le, Q.V. EfficientNet: Rethinking model scaling for convolutional neural networks. In Proceedings of the ICML, Long Beach, CA, USA, 9--15 June 2019.

\bibitem{groh2021}
Groh, M.; Harris, C.; Soenksen, L.; Lau, F.; Han, R.; Kim, A.; Koochek, A.; Badri, O. Evaluating deep neural networks trained on clinical images in dermatology with the Fitzpatrick 17k dataset. In Proceedings of the CVPR Workshops, Virtual, 19--25 June 2021.

\bibitem{hardt2016}
Hardt, M.; Price, E.; Srebro, N. Equality of opportunity in supervised learning. In Proceedings of the NeurIPS, Barcelona, Spain, 5--10 December 2016.

\bibitem{chawla2002}
Chawla, N.V.; Bowyer, K.W.; Hall, L.O.; Kegelmeyer, W.P. SMOTE: Synthetic minority over-sampling technique. \textit{J. Artif. Intell. Res.} \textbf{2002}, \textit{16}, 321--357.

\bibitem{codella2018}
Codella, N.C.F.; Gutman, D.; Celebi, M.E.; Helba, B.; Marchetti, M.A.; Dusza, S.W.; Kalloo, A.; Liopyris, K.; Mishra, N.; Kittler, H.; et al. Skin lesion analysis toward melanoma detection: A challenge at the 2017 ISBI. In Proceedings of the IEEE ISBI, Washington, DC, USA, 4--7 April 2018.

\bibitem{wong2016}
Wong, S.C.; Gatt, A.; Stamatescu, V.; McDonnell, M.D. Understanding data augmentation for classification: when to warp? In Proceedings of the DICTA, Gold Coast, Australia, 30 November--2 December 2016.

\bibitem{cui2019}
Cui, Y.; Jia, M.; Lin, T.-Y.; Song, Y.; Belongie, S. Class-balanced loss based on effective number of samples. In Proceedings of the IEEE CVPR, Long Beach, CA, USA, 15--20 June 2019.

\bibitem{ganin2016}
Ganin, Y.; Ustinova, E.; Ajakan, H.; Germain, P.; Larochelle, H.; Laviolette, F.; Marchand, M.; Lempitsky, V. Domain-adversarial training of neural networks. \textit{J. Mach. Learn. Res.} \textbf{2016}, \textit{17}, 2096--2030.

\bibitem{goodfellow2014}
Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.; Warde-Farley, D.; Ozair, S.; Courville, A.; Bengio, Y. Generative adversarial nets. In Proceedings of the NeurIPS, Montreal, QC, Canada, 8--13 December 2014.

\bibitem{wolterink2017}
Wolterink, J.M.; Dinkla, A.M.; Savenije, M.H.F.; Seevinck, P.R.; van den Berg, C.A.T.; I\v{s}gum, I. Deep MR to CT synthesis using unpaired data. In Proceedings of the SASHIMI, Quebec City, QC, Canada, 14 September 2017.

\bibitem{shaban2019}
Shaban, M.T.; Baur, C.; Navab, N.; Albarqouni, S. StainGAN: Stain style transfer for digital histopathology images using cycle-consistent generative adversarial networks. In Proceedings of the IEEE ISBI, Venice, Italy, 8--11 April 2019.

\bibitem{frid2018}
Frid-Adar, M.; Diamant, I.; Klang, E.; Amitai, M.; Goldberger, J.; Greenspan, H. GAN-based synthetic medical image augmentation for increased CNN performance in liver lesion classification. \textit{Neurocomputing} \textbf{2018}, \textit{321}, 321--331.

\bibitem{mahapatra2019}
Mahapatra, D.; Bozorgtabar, B.; Garnavi, R. Image super-resolution using progressive generative adversarial networks for medical image analysis. \textit{Comput. Med. Imaging Graph.} \textbf{2019}, \textit{71}, 30--39.

\bibitem{karras2019}
Karras, T.; Laine, S.; Aila, T. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE CVPR, Long Beach, CA, USA, 15--20 June 2019.

\bibitem{ho2020}
Ho, J.; Jain, A.; Abbeel, P. Denoising diffusion probabilistic models. In Proceedings of the NeurIPS, Virtual, 6--12 December 2020.

\bibitem{sutton2020}
Sutton, R.T.; Pincock, D.; Baumgart, D.C.; Sadowski, D.C.; Fedorak, R.N.; Kroeker, K.I. An overview of clinical decision support systems: benefits, risks, and strategies for success. \textit{NPJ Digit. Med.} \textbf{2020}, \textit{3}, 1--10.

\bibitem{singhal2023}
Singhal, K.; Azizi, S.; Tu, T.; Mahdavi, S.S.; Wei, J.; Chung, H.W.; Scales, N.; Tanwani, A.; Cole-Lewis, H.; Pfohl, S.; et al. Large language models encode clinical knowledge. \textit{Nature} \textbf{2023}, \textit{620}, 172--180.

\bibitem{siegel2023}
Siegel, R.L.; Miller, K.D.; Wagle, N.S.; Jemal, A. Cancer statistics, 2023. \textit{CA Cancer J. Clin.} \textbf{2023}, \textit{73}, 17--48.

\bibitem{lee1997}
Lee, T.; Ng, V.; Gallagher, R.; Coldman, A.; McLean, D. DullRazor: A software approach to hair removal from images. \textit{Comput. Biol. Med.} \textbf{1997}, \textit{27}, 533--543.

\bibitem{kassem2020}
Kassem, M.A.; Hosny, K.M.; Fouad, M.M. Skin lesions classification into eight classes for ISIC 2019 using deep convolutional neural network and transfer learning. \textit{IEEE Access} \textbf{2020}, \textit{8}, 114822--114832.

\bibitem{who2020}
World Health Organization. Global Strategy on Digital Health 2020--2025; WHO: Geneva, Switzerland, 2021.

\bibitem{selvaraju2017}
Selvaraju, R.R.; Cogswell, M.; Das, A.; Vedantam, R.; Parikh, D.; Batra, D. Grad-CAM: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE ICCV, Venice, Italy, 22--29 October 2017.

\bibitem{radford2015}
Radford, A.; Metz, L.; Chintala, S. Unsupervised representation learning with deep convolutional generative adversarial networks. \textit{arXiv} \textbf{2015}, arXiv:1511.06434.

\bibitem{karras2020}
Karras, T.; Laine, S.; Aittala, M.; Hellsten, J.; Lehtinen, J.; Aila, T. Analyzing and improving the image quality of StyleGAN. In Proceedings of the IEEE CVPR, Seattle, WA, USA, 13--19 June 2020.

\bibitem{chardon1991}
Chardon, A.; Cretois, I.; Hourseau, M.F. Skin colour typology and suntanning pathways. \textit{Int. J. Cosmet. Sci.} \textbf{1991}, \textit{13}, 191--208.

\bibitem{glocker2023}
Glocker, B.; Jones, C.; Bernstein, M.; Winzeck, S. Risk of bias in chest radiography deep learning foundation models. \textit{Radiol. Artif. Intell.} \textbf{2023}, \textit{5}, e230060.

\bibitem{puyol2021}
Puyol-Ant\'{o}n, E.; Ruijsink, B.; Sidhu, B.S.; Gould, J.; Porter, B.; Elliott, M.K.; Mehta, V.; Rinaldi, C.A.; Cowie, M.R.; Razavi, R.; et al. Fairness in cardiac magnetic resonance imaging: Assessing sex and racial bias in deep learning-based segmentation. \textit{Front. Cardiovasc. Med.} \textbf{2021}, \textit{8}, 669096.

\bibitem{snell2017}
Snell, J.; Swersky, K.; Zemel, R. Prototypical networks for few-shot learning. In Proceedings of the NeurIPS, Long Beach, CA, USA, 4--9 December 2017.

\bibitem{finn2017}
Finn, C.; Abbeel, P.; Levine, S. Model-agnostic meta-learning for fast adaptation of deep networks. In Proceedings of the ICML, Sydney, Australia, 6--11 August 2017.

\end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\PublishersNote{}
\end{adjustwidth}
\end{document}
